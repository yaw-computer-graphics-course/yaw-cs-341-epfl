<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="generator" content="pandoc">
    <meta name="description" content="">

    <title>Final Project Report CS-341 2025</title>

    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="css/bootstrap.min.css">

    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <link href="ie10-viewport-bug-workaround.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="css/dashboard.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <style type="text/css">code{white-space: pre;}</style>
    <style type="text/css">.sidebar ul{padding-left: 10px;}</style>
    <link rel="stylesheet" href="css/cg_report.css" />
  </head>

  <body>

    <div class="container-fluid">
      <div class="row">
        <div id="sidebar" class="col-sm-3 col-md-2 sidebar">
          <!--<ul class="nav nav-sidebar">
            <li class="active"><a href="#">Overview <span class="sr-only">(current)</span></a></li>
          </ul>-->
          <ul>
          <li><a href="#a-campfire-at-midnight"
          id="toc-a-campfire-at-midnight">A Campfire At Midnight</a>
          <ul>
          <li><a href="#abstract" id="toc-abstract">Abstract</a></li>
          <li><a href="#overview" id="toc-overview">Overview</a></li>
          <li><a href="#feature-validation"
          id="toc-feature-validation">Feature validation</a>
          <ul>
          <li><a href="#feature-1-procedurally-generated-texture"
          id="toc-feature-1-procedurally-generated-texture">Feature 1 :
          Procedurally Generated Texture</a></li>
          <li><a href="#feature-2-bloom-effect-shader"
          id="toc-feature-2-bloom-effect-shader">Feature 2 : Bloom
          Effect Shader</a></li>
          <li><a href="#feature-3-soft-shadows"
          id="toc-feature-3-soft-shadows">Feature 3 : Soft
          Shadows</a></li>
          <li><a href="#feature-4-screen-space-ambient-occulsion-ssao"
          id="toc-feature-4-screen-space-ambient-occulsion-ssao">Feature
          4 : Screen Space Ambient Occulsion (SSAO)</a></li>
          <li><a href="#feature-5-l-systems"
          id="toc-feature-5-l-systems">Feature 5 : L-Systems</a></li>
          <li><a href="#feature-6-mesh-scene-design"
          id="toc-feature-6-mesh-scene-design">Feature 6: Mesh / Scene
          Design</a></li>
          </ul></li>
          <li><a href="#discussion" id="toc-discussion">Discussion</a>
          <ul>
          <li><a href="#additional-components"
          id="toc-additional-components">Additional Components</a></li>
          <li><a href="#failed-experiments"
          id="toc-failed-experiments">Failed Experiments</a></li>
          <li><a href="#challenges"
          id="toc-challenges">Challenges</a></li>
          </ul></li>
          <li><a href="#contributions"
          id="toc-contributions">Contributions</a></li>
          <li><a href="#references" id="toc-references">References</a>
          <ul>
          <li><a href="#books" id="toc-books">üìö
          <strong>Books</strong></a></li>
          <li><a href="#blogs" id="toc-blogs">üåê
          <strong>Blogs</strong></a></li>
          <li><a href="#online-tutorials" id="toc-online-tutorials">üéì
          <strong>Online Tutorials</strong></a></li>
          <li><a href="#meshes-and-textures"
          id="toc-meshes-and-textures">üñºÔ∏è <strong>Meshes and
          Textures</strong></a></li>
          </ul></li>
          </ul></li>
          </ul>
        </div>
        <div class="col-sm-9 col-sm-offset-3 col-md-10 col-md-offset-2 main">
        
<h1 id="a-campfire-at-midnight">A Campfire At Midnight</h1>
</br></br>
<div style="text-align: center;">
<video src="videos/campfire_at_midnight.mov" height="300px" autoplay loop>
</video>
</div>
<figcaption style="text-align: center;">
The flame dances gently, flickering with life ‚Äî warm, wild, and never
still.
</figcaption>
<h2 id="abstract">Abstract</h2>
<p>This report presents the development of a real-time, interactive
campfire scene built using the <strong>regl</strong> framework. The goal
was to combine several computer graphics techniques to create a visually
rich and atmospheric environment. Key features include soft shadows,
SSAO, bloom effects, procedural fire and L-System-based tree generation.
The project emphasizes both technical execution and artistic expression,
aiming to simulate a warm, immersive nighttime setting. This document
outlines the methods used, challenges faced, and solutions implemented
throughout the development process.</p>
<h2 id="overview">Overview</h2>
<p>Here are several views showcasing the scene from different camera
perspectives:</p>
<div
style="display: flex; justify-content: space-around; flex-wrap: wrap; gap: 20px;">
<div style="flex: 0 0 45%; text-align: center;">
<img src="images/view1.png" alt="Inside Scene View" style="width: 100%; max-width: 400px;">
<p>
<strong>View 1:</strong> Inside the scene, offering an immersive,
up-close experience of the environment.
</p>
</div>
<div style="flex: 0 0 45%; text-align: center;">
<img src="images/view2.png" alt="Close Side View 1" style="width: 100%; max-width: 400px;">
<p>
<strong>View 2:</strong> Near the scene along the z-axis, showing a
slightly more distant perspective.
</p>
</div>
<div style="flex: 0 0 45%; text-align: center;">
<img src="images/view3.png" alt="Close Side View 2" style="width: 100%; max-width: 400px;">
<p>
<strong>View 3:</strong> Similar distance to View 2 but from a slightly
different angle for more spatial context.
</p>
</div>
<div style="flex: 0 0 45%; text-align: center;">
<img src="images/view4.png" alt="Top-Down View" style="width: 100%; max-width: 400px;">
<p>
<strong>View 4:</strong> A top-down perspective showing the overall
layout and composition of the scene.
</p>
</div>
</div>
<h2 id="feature-validation">Feature validation</h2>
<table>
<caption>
Feature Summary
</caption>
<thead>
<tr>
<th>
Feature
</th>
<th>
Adapted Points
</th>
<th>
Status
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
Soft Shadows
</td>
<td>
10
</td>
<td style="background-color: #d4edda;">
Completed
</td>
</tr>
<tr>
<td>
SSAO
</td>
<td>
10
</td>
<td style="background-color: #d4edda;">
Completed
</td>
</tr>
<tr>
<td>
PTG
</td>
<td>
10
</td>
<td style="background-color: #d4edda;">
Completed
</td>
</tr>
<tr>
<td>
L-Systems
</td>
<td>
10
</td>
<td style="background-color: #d4edda;">
Completed
</td>
</tr>
<tr>
<td>
Mesh/Scene Design
</td>
<td>
5
</td>
<td style="background-color: #d4edda;">
Completed
</td>
</tr>
<tr>
<td>
Bloom
</td>
<td>
5
</td>
<td style="background-color: #d4edda;">
Completed
</td>
</tr>
</tbody>
</table>
<h3 id="feature-1-procedurally-generated-texture">Feature 1 :
Procedurally Generated Texture</h3>
<h4 id="implementation">Implementation</h4>
<p>To achieve a realistic flame effect in our scene, we used procedural
generation techniques to create both the flame‚Äôs mesh and texture. This
approach allows for a dynamic visual representation that simulates the
behavior of a real flame.</p>
<p><strong>-&gt; Mesh Generation and Noise Functions:</strong> <br></p>
<p>The flame mesh is generated using the <code>fire_build_mesh</code>
function, which takes into account a height map, a base flame level, and
a time parameter. The height map defines the shape of the flame, and the
noise algorithm is used to introduce variability to the flame‚Äôs
appearance.</p>
<p>To create the height map, we use the <code>create_height_map</code>
method, which generates a 2D grid of float values based on a sine-cosine
wave pattern. This pattern simulates elevation data that can be
influenced by time-dependent offsets. The resulting height map is then
used to construct a grid of vertices, where each vertex‚Äôs height is
influenced by multiple noise functions, including:</p>
<ul>
<li><strong>Base flame height</strong> from a Noise generator</li>
<li><strong>Turbulence</strong> for chaotic movement</li>
<li><strong>Detail noise</strong> for finer variations</li>
</ul>
<p>These noise functions work together to create a realistic and dynamic
flame shape. To ensure proper lighting and shading on the mesh, we
compute normals using height values from neighboring vertices.</p>
<p><strong>-&gt; Texture Application and Animation:</strong> <br></p>
<p>The flame texture is designed to reflect realistic flame colors and
is updated dynamically. We achieve this through:</p>
<ul>
<li><strong>Color Cycling</strong>: A set of predefined flame colors is
cycled through over time, creating a smooth animation. This is managed
in the <code>evolve</code> method of the flame actor, where a timer
tracks the interval between updates.</li>
<li><strong>Texture Mapping</strong>: The mesh vertices are assigned
texture coordinates, allowing the flame texture to wrap around the
procedural mesh correctly. This involves scaling and mapping coordinates
based on the vertex positions.</li>
</ul>
<p><strong>-&gt; Real-Time Updates and Flickering Behavior:</strong>
<br></p>
<p>To simulate the flickering behavior of real flames, we update the
mesh and texture of the flame in real-time. The
<code>initialize_flame</code> method initially creates the flame object
and establishes its dynamic properties. The
<code>update_flame_light</code> function tracks the maximum elevation of
the flame to adjust the position of light sources, creating a realistic
flickering effect.</p>
<p>The <code>evolve</code> function, called every frame, updates the
height map using sine and cosine functions to create animated motion. A
new flame mesh is then generated based on the modified height map,
ensuring that the visual representation appears organic and
responsive.</p>
<p>To maintain performance while still achieving visual realism, we used
a simple animated texture with color cycling instead of generating
complex procedural textures. The noise-based mesh already gave a
satisfying flame shape and motion, so the cycling colors were enough to
create the desired flame look without adding extra complexity.</p>
<h4 id="validation">Validation</h4>
<p>To validate the implementation of the fire, we present two
videos:</p>
<ul>
<li>The first video shows only the <strong>fire plane with the texture
applied</strong>.</li>
<li>The second video shows the <strong>full animated fire</strong>,
including the procedurally generated mesh and dynamic color-cycled
texture.</li>
</ul>
<p></br></p>
<div style="text-align: center;">
<video src="videos/plane_flame_validation.mov" height="300px" autoplay loop>
</video>
</div>
<figcaption style="text-align: center;">
The textured plane shows the base flame texture without mesh
deformation.
</figcaption>
<p></br></p>
<div style="text-align: center;">
<video src="videos/flame_validation.mov" height="300px" autoplay loop>
</video>
</div>
<figcaption style="text-align: center;">
The animated flame, with both procedural mesh and dynamic texture,
demonstrating realistic flickering.
</figcaption>
<p></br></p>
<p>These two videos demonstrate the layered approach taken to achieve a
realistic flame:</p>
<ol type="1">
<li><p><strong>Texture-Only Plane</strong>:<br />
The first video validates that the flame texture is properly designed,
mapped, and animated. It shows how the procedural color cycling
contributes to visual liveliness, even before applying geometry-based
distortion.</p></li>
<li><p><strong>Full Flame with Mesh and Texture</strong>:<br />
The second video showcases the procedural generation of the flame mesh,
driven by noise and height maps. The deformation gives the flame volume
and realistic motion. Combined with the animated texture, this creates a
fire effect that flickers and shifts in real-time.</p></li>
</ol>
<p>This separation makes it clear how both components‚Äîtexture and
mesh‚Äîwork together to simulate a flame in motion.</p>
<h3 id="feature-2-bloom-effect-shader">Feature 2 : Bloom Effect
Shader</h3>
<h4 id="implementation-1">Implementation</h4>
<p>A bloom effect was implemented using a fragment shader
(<code>bloom.frag.glsl</code>) to simulate light diffusion around
high-intensity areas. This effect is commonly used to approximate the
scattering of light in real-world optics and to improve visual clarity
of bright regions.</p>
<p><strong>-&gt; Brightness Detection and Thresholding:</strong>
<br></p>
<p>The shader samples color data from the scene texture and computes
luminance for each pixel using a weighted sum of the RGB components. A
brightness threshold (<code>u_brightness_threshold</code>) is applied to
determine which pixels are considered bright. Only these pixels are
selected for further processing to minimize computational overhead.</p>
<p><strong>-&gt; Gaussian Blur Application:</strong> <br></p>
<p>For pixels that exceed the threshold, a Gaussian blur is applied
using a two-dimensional kernel. The blur is performed by sampling
neighboring pixels within a defined radius (<code>u_blurRadius</code>).
Each sample is weighted by its distance from the center pixel using a
Gaussian distribution. This step approximates the light diffusion
typically observed around bright light sources.</p>
<p><strong>-&gt; Normalization and Final Composition:</strong> <br></p>
<p>After accumulating the blurred values, the result is normalized by
dividing by the sum of the sample weights. This prevents undesired
artifacts such as dimming or dark borders. The bloom contribution is
then added to the original pixel color, scaled by a user-defined factor
(<code>u_bloom_intensity</code>) to allow control over the effect‚Äôs
strength.</p>
<p><strong>-&gt; Dynamic Integration and User Controls:</strong>
<br></p>
<p>The bloom effect is integrated into the rendering pipeline and can be
toggled via user controls in real time.</p>
<h4 id="validation-1">Validation</h4>
<p></br></p>
<div style="text-align: center;">
<video src="videos/Bloom_Validation.mp4" height="200px" autoplay loop muted></video>
<p>
Bloom Effect Validation on Fire Animation
</p>
</div>
<p>The video demonstrates the effect of bloom on a fire animation by
comparing the same scene with and without the bloom effect applied. The
bloom visually enhances the fire, creating a glowing aura around the
flame that mimics how bright light scatters, adding realism and
intensity to the scene. This validation is effective because there is a
light source positioned at the top of the flame, which naturally emits
bright light that the bloom effect can accentuate. Without bloom, the
fire looks flat and less vibrant, whereas with bloom, the light from the
flame appears to glow and softly bleed into the surrounding space,
confirming that the bloom shader is correctly simulating the expected
light diffusion.</p>
<p>We chose to show the bloom effect in this way because comparing the
same scene side-by-side with and without bloom provides a clear and
immediate visual contrast, making the impact of the effect easy to
observe and understand. Additionally, using the fire animation as the
subject is convenient because the flame is positioned at the center of
the scene, allowing the bloom effect to not only enhance the flame
itself but also influence the surrounding objects with a soft light
impact.</p>
<h3 id="feature-3-soft-shadows">Feature 3 : Soft Shadows</h3>
<h4 id="implementation-2">Implementation</h4>
<p>Soft shadow rendering is implemented using a combination of
Percentage Closer Filtering (PCF) and cube mapping. This approach
reduces hard shadow edges and produces more gradual light occlusion
transitions.</p>
<p><strong>-&gt; Cube Mapping for Shadow Capture:</strong> <br></p>
<p>To capture the necessary depth information for shadow determination,
the scene is rendered from the light source‚Äôs perspective using cube
mapping. This is handled by the <code>EnvironmentCapture</code> class,
which generates a cube map texture. The
<code>compute_shadow_cube_map</code> function manages this process,
capturing six views corresponding to each face of the cube. The
resulting cube map encodes depth values, enabling shadow visibility
calculations for fragments in all directions relative to the light
source.</p>
<p><strong>-&gt; Fragment Shader for Shadow Mapping:</strong> <br></p>
<p>In the fragment shader, each fragment‚Äôs world position is converted
into light space to determine its relationship to the light source. The
shader computes the direction vector from the fragment to the light, and
the distance between the two. These values are used to test shadow
visibility against the depth information stored in the cube map.</p>
<p><strong>-&gt; Poisson Disk Sampling:</strong> <br></p>
<p>To produce soft shadow edges, the shader applies Poisson disk
sampling with 16 offset vectors. These offsets are used to sample the
shadow map at slightly varied directions around the original light
vector. The offsets are scaled by a configurable
<code>shadow_softness</code> parameter to control the shadow blur
radius. This multi-sample approach helps reduce aliasing and creates
smoother shadow boundaries.</p>
<p><strong>-&gt; Shadow Testing:</strong> <br></p>
<p>For each Poisson disk sample, the shader retrieves the corresponding
depth from the cube shadow map and compares it to the fragment-to-light
distance. If the sampled depth is less than the actual distance, the
fragment is considered to be in shadow for that sample. This process is
repeated across all samples, with the results accumulated to compute
overall shadow intensity.</p>
<p><strong>-&gt; Averaging and Final Output:</strong> <br></p>
<p>The accumulated shadow values are averaged to generate the final
shadow factor. This averaging blends the transitions between fully lit
and fully shadowed regions, distinguishing soft shadows from hard-edged
shadows. The resulting shadow factor is used to adjust the fragment
color and lighting intensity. The system supports accumulation from
multiple light sources, ensuring consistent blending of shadow effects
across different illumination contributions.</p>
<h4 id="validation-2">Validation</h4>
<p></br></p>
<div style="text-align: center;">
<video src="videos/Soft_Shadows_Validation.mp4" height="200px" autoplay loop muted></video>
<p>
Soft Shadows Toggle Demonstration
</p>
</div>
<p></br></p>
<p>
This video showcases the soft shadow feature being toggled on and off.
When enabled, the shadows exhibit smooth edges and gradual falloff. The
clear visual difference validates the correct implementation of the soft
shadow algorithm by highlighting how it improves depth perception and
overall scene realism ‚Äî even if some visual artifacts are present, as
noted in the failed experiences section.
</p>
<p>
To better demonstrate the effect of the soft shadows, we chose to render
three UV spheres of different colors within the scene. These spheres
provide distinct and simple geometric forms that clearly reveal the
shadow softness and edge transitions. The contrasting colors help to
visually isolate each sphere‚Äôs shadow, making it easier to observe how
the soft shadow algorithm interacts with varied lighting conditions and
surface orientations. This choice enhances the clarity of the
demonstration.
</p>
<h3 id="feature-4-screen-space-ambient-occulsion-ssao">Feature 4 :
Screen Space Ambient Occulsion (SSAO)</h3>
<h4 id="implementation-3">Implementation</h4>
<p>This technique adds subtle shadows in crevices and contact areas by
analyzing nearby geometry in screen space, enhancing the perception of
depth and detail in rendered scenes without the computational cost of
full global illumination. Our SSAO implementation makes use of multiple
passes : positions, normals, ssao, ssao blur.<br></p>
<p><strong>-&gt; Positions pass:</strong> <br></p>
<p>In this pass, for each pixel, the fragment‚Äôs view-space position is
computed by multiplying each vertex position by the model-view matrix
and interpolating it. It is then stored in a texture.</p>
<p><strong>-&gt; Normals pass:</strong><br></p>
<p>Similar to the positions‚Äô pass, each fragment‚Äôs view-space normal is
computed in the vertex shader, this time by using the normals‚Äô
model-view matrix, and then interpolated and encoded to fit in the [0.0,
1.0] range.</p>
<p><strong>-&gt; SSAO pass:</strong> <br></p>
<p>The most relevant part is the fragment shader which takes as inputs
:</p>
<p><code>uv</code> : Texture coordinates for the current fragment,
interpolated from the vertex shader.</p>
<p><code>positions_tex</code>: A texture storing view-space positions of
fragments.</p>
<p><code>normals_tex</code> : A texture storing normals, encoded in the
[0, 1] range.</p>
<p><code>noise_tex</code> : A small noise texture used to randomize the
sampling kernel orientation per fragment. This texture is tiled across
the screen and is computed in the CPU.</p>
<p><code>kernel[64]</code> : A uniform array of 64 precomputed sample
vectors distributed within a hemisphere. They are used to probe the
surrounding geometry. This is compued in the CPU.</p>
<p><code>noise_scale</code> : A scale factor used to tile the noise
texture across the screen.</p>
<p><code>mat_projection</code> : The projection matrix, used to
transform view-space coordinates to screen-space.</p>
<p>It also makes use of some constants to adapt as we see fit.</p>
<p><code>bias</code> : Helps to prevent self-occlusion artifacts by
offsetting the sample depth slightly.</p>
<p><code>radius</code> : Defines the sampling radius in view space.</p>
<p><code>kernelSize</code> : Number of samples in the occlusion
kernel.</p>
<p>The algotithm goes as follows : - We start by extracting the view
space position and normal from the corresponding buffers, as well as a
random vector from the noise buffer.</p>
<ul>
<li><p>We then create a TBN matrix which stands for
tangent-bitangent-normal, where we use the Gramm-Schmidt process to
create an orthogonal basis of the tangent space. This matrix serves to
transform vectors from this space to view space.</p></li>
<li><p>Then we loop over our samples, multiply them by the TBN matrix
and adding them to the current fragment‚Äôs position.</p></li>
<li><p>Now we project it to screen-space and transform the coordinates
to the [0, 1] range to use them to sample the position texture. The
sample depth then is just the z component of the resulting
vector.</p></li>
<li><p>Next we use an occlusion variable to cumulate the total fragment
occlusion, this variable gets incremented or not for each sample by
comparing its depth.</p></li>
<li><p>The final value is then inverted and normalized and output as the
fragment ‚Äúcolor‚Äù in this pass.</p></li>
</ul>
<p><strong>-&gt; Blur pass:</strong> <br></p>
<p>The raw SSAO texture tends to be noisy due to the random sampling. To
clean it up, a box blur is applied over a small grid (4x4) of
neighboring pixels. Each neighboring SSAO value is averaged to smooth
out artifacts. The result is a softened occlusion texture that still
retains detail but avoids pixelated noise.</p>
<p><strong>-&gt; Final output (Integration in the map
mixer):</strong><br></p>
<p>After computing the previous passes, the resulting buffer gets passed
to the map mixer shader which essentially adds the ssao factor to the
final color by simply multiplying it.</p>
<h4 id="validation-3">Validation</h4>
<p>To validate the SSAO implementation, we present the key intermediate
buffers and final results below. Each step confirms that the underlying
data and operations function as intended.</p>
<div style="text-align: center;">
<figure style="display: inline-block; margin: 10px;">
<img src="images/positions.png" height="200px" width="250px" style="vertical-align: middle;">
<figcaption>
Position buffer
</figcaption>
</figure>
<figure style="display: inline-block; margin: 10px;">
<img src="images/normals.png" height="200px" width="250px" style="vertical-align: middle;">
<figcaption>
Normal buffer
</figcaption>
</figure>
<figure style="display: inline-block; margin: 10px;">
<img src="images/raw_ssao.png" height="200px" width="250px" style="vertical-align: middle;">
<figcaption>
Raw SSAO output
</figcaption>
</figure>
<figure style="display: inline-block; margin: 10px;">
<img src="images/blur_ssao.png" height="200px" width="250px" style="vertical-align: middle;">
<figcaption>
Blurred SSAO output
</figcaption>
</figure>
</div>
<p><br></p>
<div style="text-align: center;">
<p>
<strong>Visual Comparison of SSAO Enabled vs.¬†Disabled</strong>
</p>
<p><video src="videos/SSAO_Validation.mp4" height="300px" autoplay loop muted></video></p>
</div>
<p>This video demonstrates the effect of enabling SSAO. Without SSAO,
objects appear uniformly lit and disconnected from the ground,
especially at contact points. When SSAO is enabled, subtle shadows form
around the base of the model, adding depth and realism to the
lighting.</p>
<p><br></p>
<div style="text-align: center;">
<p>
<strong>Static Scene Comparison</strong>
</p>
<figure style="display: inline-block; margin: 10px;">
<img src="images/without_ssao.png" height="200px" width="503px" style="vertical-align: middle;">
<figcaption>
Scene without SSAO ‚Äî lighting appears flatter and less integrated with
geometry.
</figcaption>
</figure>
<figure style="display: inline-block; margin: 10px;">
<img src="images/with_ssao.png" height="200px" width="503px" style="vertical-align: middle;">
<figcaption>
Scene with SSAO ‚Äî subtle ambient shadows enhance realism, particularly
near contact surfaces.
</figcaption>
</figure>
</div>
<p>By comparing the two images, the effect of SSAO can be observed. When
SSAO is enabled, additional ambient shadows appear around the base of
the trees and logs, helping to define their spatial relationship with
the ground. This contributes to a lighting result that appears more
integrated with the environment.</p>
<h3 id="feature-5-l-systems">Feature 5 : L-Systems</h3>
<h4 id="implementation-4">Implementation</h4>
<p>This implementation generates a 3D procedural tree mesh using an
L-system and simulates plant-like branching structures. It uses two main
things: an axiom and a set of rules. From the axiom, it recursively
replaces each element based on the rule set until it reaches the
specified number of iterations. It then draws using a turtle based
algorithm, the turtle keeps a state throught the drawing process. This
state contains its positions, three vectors that represent a moving
orthogonal basis, the current thickness and height of the cylinder as
well the last ring for optimization purposes.</p>
<p>The L-Systems rules contain some special characters :</p>
<p><code>[</code> : Saves the current state of the turtle into a
stack.</p>
<p><code>]</code> : Pops the previously saved state from the stack.</p>
<p><code>+</code> : Rotates the angle along +x.</p>
<p><code>-</code> : Rotates the angle along -x.</p>
<p><code>^</code> : Rotates the angle along +y.</p>
<p><code>&amp;</code> : Rotates the angle along -y.</p>
<p>Then, for each character not among these, the turtle draws a cylinder
by computing the center points and surrounding circle points then
extracts the faces. And for each cylinder, it has a probability to draw
a branch with is just a generated simple triangle mesh for randomness.
Then we compute normals using triangle normals and angle weights using
the same method as the previous assignments.</p>
<h4 id="validation-4">Validation</h4>
<p>Here is some outputs for different iterations:</p>
<div style="text-align: center;">
<!-- L-System Tree Iterations -->
<figure style="display: inline-block; margin: 10px;">
<img src="images/tree-iter-1.png" height="200px" width="250px" style="vertical-align: middle;">
<figcaption>
Tree generated using L-System - Iteration 1
</figcaption>
</figure>
<figure style="display: inline-block; margin: 10px;">
<img src="images/tree-iter-2.png" height="200px" width="250px" style="vertical-align: middle;">
<figcaption>
Tree generated using L-System - Iteration 2
</figcaption>
</figure>
<figure style="display: inline-block; margin: 10px;">
<img src="images/tree-iter-3.png" height="200px" width="250px" style="vertical-align: middle;">
<figcaption>
Tree generated using L-System - Iteration 3
</figcaption>
</figure>
<figure style="display: inline-block; margin: 10px;">
<img src="images/tree-iter-4.png" height="200px" width="250px" style="vertical-align: middle;">
<figcaption>
Tree generated using L-System - Iteration 4
</figcaption>
</figure>
</div>
<br>
<p style="text-align: center;">
Below are examples of popular L-System based fractals:
</p>
<p><br></p>
<div style="text-align: center;">
<figure style="display: inline-block; margin: 10px;">
<img src="images/koch-snowflake.png" height="200px" width="250px" style="vertical-align: middle;">
<figcaption>
Koch Snowflake - A classic fractal generated using an L-System
</figcaption>
</figure>
<figure style="display: inline-block; margin: 10px;">
<img src="images/sierpinski-triangle.png" height="200px" width="250px" style="vertical-align: middle;">
<figcaption>
Sierpi≈Ñski Triangle - Another well-known L-System fractal
</figcaption>
</figure>
</div>
<h3 id="feature-6-mesh-scene-design">Feature 6: Mesh / Scene Design</h3>
<h4 id="implementation-5">Implementation</h4>
<p>The mesh and scene design played a central role in constructing the
visual environment of <strong>‚ÄúA Campfire At Midnight.‚Äù</strong> The
scene was built using a combination of procedural generation and manual
modeling to create a cohesive and immersive setting.</p>
<h4 id="mesh-design">Mesh Design</h4>
<p><strong>Procedural Terrain Generation</strong><br />
The terrain was procedurally generated using noise functions to form a
floating island, serving as the foundation for all other scene elements.
This approach allowed for natural variation in the landscape while
maintaining a stylized aesthetic.</p>
<p><strong>Flame Mesh</strong><br />
The campfire flame was designed using a procedurally generated mesh,
enhanced with animated textures. These elements worked together to
simulate flame motion and depth, resulting in a more dynamic and
engaging visual effect.</p>
<p><strong>Environment Elements</strong><br />
Objects such as rocks, logs, and vegetation were added to support the
campfire setting. These elements were positioned to align with the
terrain and were textured to maintain visual consistency throughout the
scene.</p>
<h4 id="scene-design">Scene Design</h4>
<p><strong>Atmospheric Lighting</strong><br />
A dynamic lighting system was used to simulate the warm illumination of
a campfire. The light source was programmed to adjust its position in
real time, always aligning with the highest elevation point of the
flame. This setup produced soft shadows and interacted with nearby
objects to enhance the perception of depth in the scene.</p>
<h4 id="validation-5">Validation</h4>
<p></br></p>
<div style="text-align: center;">
<img src="images/view2.png" alt="Bottom view of procedurally generated sky island terrain" height="200px"/>
<p>
Bottom View of Procedurally Generated Sky Island Terrain
</p>
</div>
<p></br></p>
<div style="text-align: center;">
<video src="videos/campfire_at_midnight.mov" height="200px" autoplay loop muted></video>
<p>
Full Scene Overview Showing Procedural Meshes and Textures
</p>
</div>
<p></br></p>
<p>The image shows the bottom part of the sky island, highlighting the
procedurally generated mesh terrain that forms the island‚Äôs foundation.
This terrain is built dynamically using a procedural height map,
ensuring natural and varied elevation patterns.</p>
<p>The video presents the entire scene, showcasing the procedural meshes
with their corresponding textures, such as logs, stones, benches, chest,
branches, firewood, and others, integrated seamlessly into the
environment. The scene includes detailed static objects and dynamic
elements like the animated flame, all composed and textured using
materials managed via the resource manager. The procedural generation
techniques provide both the landscape and the objects populating it.</p>
<p>Some meshes and textures used in the scene have been sourced from
free platforms. References and credits for these assets are provided at
the bottom of the report.</p>
<h2 id="discussion">Discussion</h2>
<h3 id="additional-components">Additional Components</h3>
<h4 id="flame-generation-approach">Flame Generation Approach</h4>
<p>Although the original approach focused only on using procedurally
generated textures for the flame, it was later decided to also
incorporate a procedurally generated mesh. This combination provided
greater visual depth and contributed to a more dynamic and realistic
flame effect. For a more technical explanation, refer to <strong>Feature
1</strong>.</p>
<h3 id="failed-experiments">Failed Experiments</h3>
<p>Despite progress, two major issues with soft shadow rendering remain
unresolved.</p>
<h4 id="issues-in-soft-shadow-implementation">Issues in Soft Shadow
Implementation</h4>
<p>During the development of soft shadow rendering, two primary visual
issues were encountered. These affect the consistency and realism of
shadows across different lighting conditions and viewing angles.</p>
<p><strong>-&gt; 1. Inconsistent Shadow Rendering Near Light
Sources:</strong> <br></p>
<p>In some situations‚Äîespecially around bright light sources like the
flame of the campfire and from certain camera angles‚Äîsoft shadows were
missing or appeared unnaturally sharp. This led to inconsistent lighting
and a less realistic appearance.</p>
<p></br></p>
<div style="text-align:center; margin: 20px 0;">
<img src="images/failed_exp_circle.png" width="60%" />
<figcaption>
Figure: Missing soft shadows near the campfire light source.
</figcaption>
</div>
<p></br></p>
<p><strong>Potential Causes:</strong></p>
<ul>
<li><strong>Cube Mapping Limitations</strong>: Fragments near the edges
of the cube map‚Äôs coverage may lack sufficient detail.</li>
<li><strong>Poisson Disk Sampling Variation</strong>: The shadow
softness depends on the sampling distribution, which may produce
inconsistent results depending on the view angle and location.</li>
</ul>
<p><strong>Potential Mitigation Strategies:</strong></p>
<ul>
<li><strong>Increased Sampling Resolution</strong>: More Poisson disk
samples can produce smoother shadow edges, at a higher computational
cost.</li>
<li><strong>Adjustable Shadow Softness</strong>: Dynamically modifying
the <code>shadow_softness</code> parameter, possibly based on distance
to the light source.</li>
<li><strong>Bias Optimization</strong>: Carefully adjusting the bias
used in depth comparisons to reduce shadow acne and artifacts.</li>
<li><strong>Hybrid Shadow Techniques</strong>: Mixing soft and hard
shadows to balance realism and detail near complex lighting
regions.</li>
<li><strong>Higher Cube Map Resolution</strong>: Improves detail capture
and helps reduce shadow loss near bright or complex surfaces.</li>
</ul>
<p><strong>-&gt; 2. Artifacts at Greater Distances:</strong> <br></p>
<p>On distant terrain surfaces, especially in shadowed regions, visual
artifacts such as black lines were visible. These disrupted the visual
flow and were particularly noticeable in low-light conditions.</p>
<p></br></p>
<div style="text-align:center; margin: 20px 0;">
<img src="images/failed_exp_dark_lines.png" width="60%" />
<figcaption>
Figure: Black line artifacts appearing in distant shadows on terrain
surfaces.
</figcaption>
</div>
<p></br></p>
<p><strong>Potential Causes:</strong></p>
<ul>
<li><strong>Depth Precision Loss</strong>: Shadow map precision
decreases with distance, leading to inaccurate shadow results.</li>
<li><strong>Insufficient Sampling Coverage</strong>: Poisson disk
samples may not provide adequate variation at longer distances.</li>
<li><strong>Fixed Bias Limitations</strong>: A uniform bias does not
adapt well to varying fragment depths, resulting in incorrect
shadowing.</li>
</ul>
<p><strong>Potential Solutions:</strong></p>
<ul>
<li><strong>Higher Shadow Map Resolution</strong>: Improves shadow
accuracy and reduces artifacts in large or distant surfaces.</li>
<li><strong>Distance-Based Biasing</strong>: Modifying bias based on
fragment distance to improve consistency across depth ranges.</li>
<li><strong>Post-Processing Filters</strong>: Applying blur or blending
filters during post-processing to smooth out hard lines or
discontinuities.</li>
</ul>
<h3 id="challenges">Challenges</h3>
<p>Before settling on the final scene, we explored numerous different
approaches and had to backtrack several times. Here is a compilation of
some of the alternative methods and ideas that were tested but
ultimately set aside.</p>
<div
style="display: flex; flex-wrap: wrap; justify-content: center; gap: 20px;">
<div style="flex: 0 0 30%; text-align: center;">
<img src="images/failed_exp1.png" width="100%" />
<p>
Terrain generation using a weird height map created unstable and
exaggerated shapes that looked unrealistic.
</p>
</div>
<div style="flex: 0 0 30%; text-align: center;">
<img src="images/failed_exp2.png" width="100%" />
<p>
Another terrain attempt with an odd height map led to exaggerated shapes
that looked unrealistic.
</p>
</div>
<div style="flex: 0 0 30%; text-align: center;">
<img src="images/failed_exp3.png" width="100%" />
<p>
Incorrect bloom parameters resulted in overly bright highlights and loss
of visual detail.
</p>
</div>
<div style="flex: 0 0 30%; text-align: center;">
<img src="images/failed_exp4.png" width="100%" />
<p>
Sky brightness caused the bloom effect to be exaggerated, making the
upper part of the scene overly intense.
</p>
</div>
<div style="flex: 0 0 30%; text-align: center;">
<img src="images/failed_exp5.png" width="100%" />
<p>
Combining fire mesh from Blender* with dynamic properties lacked realism
and visual clarity.
</p>
</div>
<div style="flex: 0 0 30%; text-align: center;">
<img src="images/failed_exp6.png" width="100%" />
<p>
Procedural flame using noise produced awkward shapes that didn‚Äôt look
natural.
</p>
</div>
</div>
<p>* <em>We later replaced the Blender-created flame with a procedurally
generated mesh and applied a procedural texture on top, which better
captured the desired dynamic and natural appearance.</em></p>
<p>Despite the setbacks shown above, we were able to resolve all of them
through iterative improvements. We included these failed attempts in the
report as a trace of the development process and the challenges we faced
and overcame.</p>
<h2 id="contributions">Contributions</h2>
<table>
<caption>
Worked hours
</caption>
<thead>
<tr>
<th>
Name
</th>
<th>
Week 1
</th>
<th>
Week 2
</th>
<th>
Week 3
</th>
<th>
Week 4
</th>
<th>
Week 5
</th>
<th>
Week 6
</th>
<th>
Week 7
</th>
<th>
Total
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
Walid Ait Said
</td>
<td>
8
</td>
<td style="background-color: #f0f0f0;">
</td>
<td>
10
</td>
<td>
7
</td>
<td>
6
</td>
<td>
8
</td>
<td>
10
</td>
<td>
50
</td>
</tr>
<tr>
<td>
Youssef Benhayoun Sadafi
</td>
<td>
8
</td>
<td style="background-color: #f0f0f0;">
</td>
<td>
11
</td>
<td>
7
</td>
<td>
7
</td>
<td>
6
</td>
<td>
10
</td>
<td>
50
</td>
</tr>
<tr>
<td>
Adam Bekkar
</td>
<td>
8
</td>
<td style="background-color: #f0f0f0;">
</td>
<td>
10
</td>
<td>
6
</td>
<td>
8
</td>
<td>
7
</td>
<td>
10
</td>
<td>
50
</td>
</tr>
</tbody>
</table>
<table>
<caption>
Individual contributions
</caption>
<thead>
<tr>
<th>
Name
</th>
<th>
Contribution
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
Walid Ait Said
</td>
<td>
1/3
</td>
</tr>
<tr>
<td>
Youssef Benhayoun Sadafi
</td>
<td>
1/3
</td>
</tr>
<tr>
<td>
Adam Bekkar
</td>
<td>
1/3
</td>
</tr>
</tbody>
</table>
<h2 id="references">References</h2>
<h3 id="books">üìö <strong>Books</strong></h3>
<ul>
<li><strong>Marschner &amp; Shirley: <em>Fundamentals of Computer
Graphics</em>, 5th Edition, AK Peters, 2021</strong></li>
<li><strong>Glassner: <em>Graphics Gems</em>, Academic Press,
1989</strong></li>
<li><strong>Ebert, Musgrave, Peachey, Perlin, Worley: <em>Texturing
&amp; Modeling: A Procedural Approach</em>, 3rd Edition</strong></li>
</ul>
<h3 id="blogs">üåê <strong>Blogs</strong></h3>
<ul>
<li><a
href="https://github.com/lettier/3d-game-shaders-for-beginners">3D Game
Shaders for Beginners</a></li>
</ul>
<h3 id="online-tutorials">üéì <strong>Online Tutorials</strong></h3>
<ul>
<li><a
href="https://github.com/lettier/awesome-computer-graphics">Awesome
Computer Graphics Resources by lettier</a></li>
<li><a
href="https://www.youtube.com/watch?v=vLSphLtKQ0o&amp;list=PLplnkTzzqsZTfYh4UbhLGpI5kGd5oW_Hh">Introduction
to Computer Graphics by Cem Yuksel</a></li>
<li><a
href="https://youtube.com/playlist?list=PLjEaoINr3zgEPv5y--4MKpciLaoQYZB1Z&amp;si=tnK_WiwRdk_pP7cw">Blender
4.0 Beginner Donut Tutorial (Newest) by Blender Guru</a></li>
<li><a href="https://learnopengl.com/">Learn OpenGL: Learn modern OpenGL
graphics programming in a step-by-step fashion</a></li>
</ul>
<h3 id="meshes-and-textures">üñºÔ∏è <strong>Meshes and
Textures</strong></h3>
<ul>
<li><a
href="https://free3d.com/3d-model/hipster-bench-83378.html">Benches</a></li>
<li><a
href="https://www.cgtrader.com/free-3d-models/various/various-models/campfire-7c95f25c-078b-4386-b6b6-fed4b0a0375b">Campfire</a></li>
<li><a
href="https://www.freepik.com/free-photo/green-grass-texture_969018.htm#fromView=keyword&amp;page=1&amp;position=37&amp;uuid=6a92b9ae-c3d6-4912-ad3f-e0337f5b4947&amp;query=Grass+Texture">Grass</a></li>
<li><a
href="https://sketchfab.com/3d-models/medieval-chest-e84b6208cbb148ab9a32d6c52897dad1">Chest</a></li>
<li><a
href="https://www.freepik.com/free-photo/closeup-shot-gray-rough-concrete-background_9990863.htm#fromView=keyword&amp;page=1&amp;position=45&amp;uuid=ac5cc307-b3ab-4939-b72e-3ac48c98413d&amp;query=Rock+Texture">Sky
Island</a></li>
</ul>
        </div>
      </div>
    </div>
    
    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="js/jquery.min.js"></script>
		<script src="js/bootstrap.min.js"></script>
    <script>
        //document.getElementById('sidebar').getElementsByTagName('ul')[0].className += "nav nav-sidebar";
        
        /* ajust the height when click the toc
           the code is from https://github.com/twbs/bootstrap/issues/1768
        */
        var shiftWindow = function() { scrollBy(0, -50) };
        window.addEventListener("hashchange", shiftWindow);
        function load() { if (window.location.hash) shiftWindow(); }
        
        /*add Bootstrap styles to tables*/
        var tables = document.getElementsByTagName("table");
        for(var i = 0; i < tables.length; ++i){
            tables[i].className += "table table-bordered table-hover";
        }
    </script>
  </body>
</html>
